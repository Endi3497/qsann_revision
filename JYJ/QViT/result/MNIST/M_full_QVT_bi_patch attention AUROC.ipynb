{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pennylane as qml\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import inspect\n",
    "import math\n",
    "from collections.abc import Iterable\n",
    "from typing import Callable, Dict, Union, Any\n",
    "\n",
    "from pennylane import QNode\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    from torch.nn import Module\n",
    "\n",
    "    TORCH_IMPORTED = True\n",
    "except ImportError:\n",
    "    # The following allows this module to be imported even if PyTorch is not installed. Users\n",
    "    # will instead see an ImportError when instantiating the TorchLayer.\n",
    "    from unittest.mock import Mock\n",
    "\n",
    "    Module = Mock\n",
    "    TORCH_IMPORTED = False\n",
    "\n",
    "\n",
    "class TorchLayer(Module):\n",
    "    def __init__(self,qnode,weights):\n",
    "        if not TORCH_IMPORTED:\n",
    "            raise ImportError(\n",
    "                \"TorchLayer requires PyTorch. PyTorch can be installed using:\\n\"\n",
    "                \"pip install torch\\nAlternatively, \"\n",
    "                \"visit https://pytorch.org/get-started/locally/ for detailed \"\n",
    "                \"instructions.\"\n",
    "            )\n",
    "        super().__init__()\n",
    "\n",
    "        #weight_shapes = {\n",
    "        #    weight: (tuple(size) if isinstance(size, Iterable) else () if size == 1 else (size,))\n",
    "        #    for weight, size in weight_shapes.items()\n",
    "        #}\n",
    "\n",
    "        # validate the QNode signature, and convert to a Torch QNode.\n",
    "        # TODO: update the docstring regarding changes to restrictions when tape mode is default.\n",
    "        #self._signature_validation(qnode, weight_shapes)\n",
    "        self.qnode = qnode\n",
    "        self.qnode.interface = \"torch\"\n",
    "\n",
    "        self.qnode_weights = weights\n",
    "\n",
    "    def forward(self, inputs):  # pylint: disable=arguments-differ\n",
    "        \"\"\"Evaluates a forward pass through the QNode based upon input data and the initialized\n",
    "        weights.\n",
    "\n",
    "        Args:\n",
    "            inputs (tensor): data to be processed\n",
    "\n",
    "        Returns:\n",
    "            tensor: output data\n",
    "        \"\"\"\n",
    "\n",
    "        if len(inputs.shape) > 1:\n",
    "            # If the input size is not 1-dimensional, unstack the input along its first dimension,\n",
    "            # recursively call the forward pass on each of the yielded tensors, and then stack the\n",
    "            # outputs back into the correct shape\n",
    "            reconstructor = [self.forward(x) for x in torch.unbind(inputs)]\n",
    "            return torch.stack(reconstructor)\n",
    "\n",
    "        # If the input is 1-dimensional, calculate the forward pass as usual\n",
    "        return self._evaluate_qnode(inputs)\n",
    "\n",
    "\n",
    "    def _evaluate_qnode(self, x):\n",
    "        \"\"\"Evaluates the QNode for a single input datapoint.\n",
    "\n",
    "        Args:\n",
    "            x (tensor): the datapoint\n",
    "\n",
    "        Returns:\n",
    "            tensor: output datapoint\n",
    "        \"\"\"\n",
    "        kwargs = {\n",
    "            **{self.input_arg: x},\n",
    "            **{arg: weight.to(x) for arg, weight in self.qnode_weights.items()},\n",
    "        }\n",
    "        res = self.qnode(**kwargs)\n",
    "\n",
    "        if isinstance(res, torch.Tensor):\n",
    "            return res.type(x.dtype)\n",
    "\n",
    "        return torch.hstack(res).type(x.dtype)\n",
    "\n",
    "    def __str__(self):\n",
    "        detail = \"<Quantum Torch Layer: func={}>\"\n",
    "        return detail.format(self.qnode.func.__name__)\n",
    "\n",
    "    __repr__ = __str__\n",
    "    _input_arg = \"inputs\"\n",
    "\n",
    "    @property\n",
    "    def input_arg(self):\n",
    "        \"\"\"Name of the argument to be used as the input to the Torch layer. Set to ``\"inputs\"``.\"\"\"\n",
    "        return self._input_arg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "def visualize_attention(alpha, img_shape=(8,8), patch_shape=(4,4)):\n",
    "    \"\"\"\n",
    "    alpha: Attention values (torch tensor or numpy array)\n",
    "    img_shape: Shape of the original image (height, width)\n",
    "    patch_shape: Shape of the patch (height, width)\n",
    "    \"\"\"\n",
    "    # Reshape alpha to match image shape\n",
    "    alpha = alpha.detach().numpy() if isinstance(alpha, torch.Tensor) else alpha\n",
    "    attention_map = np.mean(alpha, axis=0).reshape(patch_shape)  # Mean over the sequence\n",
    "    \n",
    "    # Normalize the attention map\n",
    "    attention_map = (attention_map - attention_map.min()) / (attention_map.max() - attention_map.min())\n",
    "    \n",
    "    # Plot the attention map\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    sns.heatmap(attention_map, annot=True, cmap=\"YlGnBu\", cbar=True)\n",
    "    plt.title(\"Attention Map\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QSAL_pennylane(torch.nn.Module):\n",
    "    def __init__(self,S,n,Denc,D):\n",
    "        super().__init__()\n",
    "        self.seq_num=S\n",
    "        self.init_params_Q=torch.nn.Parameter(torch.stack([(np.pi/4) * (2 * torch.randn(n*(D+2)) - 1) for _ in range(self.seq_num)]))\n",
    "        self.init_params_K=torch.nn.Parameter(torch.stack([(np.pi/4) * (2 * torch.randn(n*(D+2)) - 1) for _ in range(self.seq_num)]))\n",
    "        self.init_params_V=torch.nn.Parameter(torch.stack([(np.pi/4) * (2 * torch.randn(n*(D+2)) - 1) for _ in range(self.seq_num)]))\n",
    "        self.num_q=n\n",
    "        self.Denc=Denc\n",
    "        self.D=D\n",
    "        self.d=n*(Denc+2)\n",
    "        self.dev = qml.device(\"default.qubit\", wires=self.num_q)\n",
    "        \n",
    "        self.vqnod=qml.QNode(self.circuit_v, self.dev, interface=\"torch\")\n",
    "        self.qnod=qml.QNode(self.circuit_qk, self.dev, interface=\"torch\")\n",
    "        self.weight_v = [{\"weights\": self.init_params_V[i]} for i in range(self.seq_num)]\n",
    "        self.weight_q = [{\"weights\": self.init_params_Q[i]} for i in range(self.seq_num)]\n",
    "        self.weight_k = [{\"weights\": self.init_params_K[i]} for i in range(self.seq_num)]\n",
    "        #self.v_linear ={} #[qml.qnn.TorchLayer(self.vqnod[i], self.weight_shapes) for i in range(self.seq_num)]\n",
    "        #for i in range(self.seq_num):\n",
    "        self.v_linear = [TorchLayer(self.vqnod, self.weight_v[i]) for i in range(self.seq_num)]\n",
    "        self.q_linear = [TorchLayer(self.qnod, self.weight_q[i]) for i in range(self.seq_num)]\n",
    "        self.k_linear = [TorchLayer(self.qnod, self.weight_k[i]) for i in range(self.seq_num)]\n",
    "        #self.qqnod=[qml.QNode(self.circuit_qk, self.dev, interface=\"torch\") for i in range(self.seq_num)]\n",
    "\n",
    "    def random_op(self):\n",
    "        a=random.randint(0, 4)\n",
    "        if a==0:\n",
    "            op=qml.Identity(0)\n",
    "        elif a==1:\n",
    "            op=qml.PauliX(0)\n",
    "        elif a==2:\n",
    "            op=qml.PauliY(0)\n",
    "        else:\n",
    "            op=qml.PauliZ(0)\n",
    "\n",
    "        op_elimated=qml.Identity(0)\n",
    "        for i in range(1,self.num_q):\n",
    "            op_elimated=op_elimated@qml.Identity(i)\n",
    "        Select_wrong=True\n",
    "        while Select_wrong:\n",
    "            for i in range(1,self.num_q):\n",
    "                a=random.randint(0, 4)\n",
    "                if a==0:\n",
    "                    op=op@qml.Identity(i)\n",
    "                elif a==1:\n",
    "                    op=op@qml.PauliX(i)\n",
    "                elif a==2:\n",
    "                    op=op@qml.PauliY(i)\n",
    "                else:\n",
    "                    op=op@qml.PauliZ(i)\n",
    "            if op!=op_elimated:\n",
    "                Select_wrong=False\n",
    "        return op\n",
    "\n",
    "    def circuit_v(self,inputs,weights):\n",
    "            op=self.random_op()\n",
    "            # feature_map\n",
    "            indx=0\n",
    "            for j in range(self.num_q):\n",
    "                qml.RX(inputs[indx],j)\n",
    "                qml.RY(inputs[indx+1],j)\n",
    "                indx+=2\n",
    "            for i in range(self.Denc):\n",
    "                for j in range(self.num_q):\n",
    "                    qml.CNOT(wires=(j,(j+1)%self.num_q))\n",
    "\n",
    "                for j in range(self.num_q):\n",
    "                    qml.RY(inputs[indx],j)\n",
    "                    indx+=1\n",
    "            # Ansatz\n",
    "            indx=0\n",
    "            for j in range(self.num_q):\n",
    "                qml.RX(weights[indx],j)\n",
    "                qml.RY(weights[indx+1],j)\n",
    "                indx+=2\n",
    "            for i in range(self.D):\n",
    "                for j in range(self.num_q):\n",
    "                    qml.CNOT(wires=(j,(j+1)%self.num_q))\n",
    "                    \n",
    "                for j in range(self.num_q):\n",
    "                    #qc.rx(params[indx],j)\n",
    "                    qml.RY(weights[indx],j)\n",
    "                    indx+=1\n",
    "            return [qml.expval(op) for i in range(self.d)] \n",
    "\n",
    "    def circuit_qk(self,inputs,weights):\n",
    "        op=self.random_op()\n",
    "        # feature_map\n",
    "        indx=0\n",
    "        for j in range(self.num_q):\n",
    "            qml.RX(inputs[indx],j)\n",
    "            qml.RY(inputs[indx+1],j)\n",
    "            indx+=2\n",
    "        for i in range(self.Denc):\n",
    "            for j in range(self.num_q):\n",
    "                qml.CNOT(wires=(j,(j+1)%self.num_q))\n",
    "\n",
    "            for j in range(self.num_q):\n",
    "                qml.RY(inputs[indx],j)\n",
    "                indx+=1\n",
    "        # Ansatz\n",
    "        indx=0\n",
    "        for j in range(self.num_q):\n",
    "            qml.RX(weights[indx],j)\n",
    "            qml.RY(weights[indx+1],j)\n",
    "            indx+=2\n",
    "        for i in range(self.D):\n",
    "            for j in range(self.num_q):\n",
    "                qml.CNOT(wires=(j,(j+1)%self.num_q))\n",
    "                \n",
    "            for j in range(self.num_q):\n",
    "                #qc.rx(params[indx],j)\n",
    "                qml.RY(weights[indx],j)\n",
    "                indx+=1\n",
    "        return [qml.expval(qml.PauliZ(0))]\n",
    "\n",
    "    def forward(self,input):\n",
    "\n",
    "        Q_output=torch.stack([self.q_linear[i](input[:,i]) for i in range(self.seq_num)])\n",
    "        K_output=torch.stack([self.k_linear[i](input[:,i]) for i in range(self.seq_num)])\n",
    "        V_output=torch.stack([self.v_linear[i](input[:,i]) for i in range(self.seq_num)])\n",
    "        \n",
    "        batch_size=len(input)\n",
    "        Q_output=Q_output.transpose(0,2).repeat((self.seq_num,1,1))\n",
    "        K_output=K_output.transpose(0,2).repeat((self.seq_num,1,1)).transpose(0,2)\n",
    "\n",
    "        #print(V_output.size())\n",
    "        #Q_grid, K_grid=torch.meshgrid(Q_output, K_output, indexing='ij')\n",
    "        alpha=torch.exp(-(Q_output-K_output)**2)\n",
    "        alpha=alpha.transpose(0,1)\n",
    "        print(f'alpha: {alpha}')\n",
    "        # Example usage:\n",
    "        visualize_attention(alpha)\n",
    "        V_output=V_output.transpose(0,1)\n",
    "        output=[]\n",
    "        \n",
    "        print(\"Q_output size:\", Q_output.size())\n",
    "        print(\"K_output size:\", K_output.size())\n",
    "        print(\"V_output size:\", V_output.size())\n",
    "\n",
    "\n",
    "        for i in range(self.seq_num):\n",
    "            \n",
    "            Sum_a=torch.sum(alpha[:,i,:],-1)\n",
    "            div_sum_a=(1/Sum_a).repeat(self.d,self.seq_num,1).transpose(0,2)\n",
    "            \n",
    "            Sum_w=torch.sum(alpha[:,:,i].repeat((self.d,1,1)).transpose(0,2).transpose(0,1)*V_output*div_sum_a,1)\n",
    "            output.append(Sum_w)\n",
    "        return input+torch.stack(output).transpose(0,1)\n",
    "\n",
    "\n",
    "class QSANN_pennylane(torch.nn.Module):\n",
    "    def __init__(self,S,n,Denc,D,num_layers):\n",
    "        \"\"\"\n",
    "        # input: input data\n",
    "        # weight: trainable parameter\n",
    "        # n: # of of qubits\n",
    "        # d: embedding dimension which is equal to n(Denc+2)\n",
    "        # Denc: the # number of layers for encoding \n",
    "        # D: the # of layers of variational layers\n",
    "        # type \"K\": key, \"Q\": Query, \"V\": value\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.qsal_lst=[QSAL_pennylane(S,n,Denc,D) for _ in range(num_layers)]\n",
    "        self.qnn=nn.Sequential(*self.qsal_lst)\n",
    "\n",
    "    def forward(self,input):\n",
    "        return self.qnn(input)\n",
    "\n",
    "class QSANN_text_classifier(torch.nn.Module):\n",
    "    def __init__(self,S,n,Denc,D,num_layers):\n",
    "        \"\"\"\n",
    "        # input: input data\n",
    "        # weight: trainable parameter\n",
    "        # n: # of of qubits\n",
    "        # d: embedding dimension which is equal to n(Denc+2)\n",
    "        # Denc: the # number of layers for encoding \n",
    "        # D: the # of layers of variational layers\n",
    "        # type \"K\": key, \"Q\": Query, \"V\": value\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.Qnn=QSANN_pennylane(S,n,Denc,D,num_layers)\n",
    "        self.final_layer=nn.Linear(n*(Denc+2)*S, 1)\n",
    "        self.final_layer=self.final_layer.float()\n",
    "\n",
    "    def forward(self,input):\n",
    "        print(input.shape)\n",
    "        x=self.Qnn(input)\n",
    "        x=torch.flatten(x,start_dim=1)\n",
    "        # print('done2')\n",
    "        output = torch.sigmoid(self.final_layer(x))\n",
    "        # print('done3')\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 1600\n",
      "Test set size: 400\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# MNIST 데이터 불러오기\n",
    "(X_train_full, y_train_full), (X_test_full, y_test_full) = mnist.load_data()\n",
    "\n",
    "# 데이터 정규화 (0-255 값을 0-1 사이로)\n",
    "X_train_full = X_train_full.astype('float32') / 255.0\n",
    "X_test_full = X_test_full.astype('float32') / 255.0\n",
    "\n",
    "# 숫자 1과 7만 선택하는 마스크 생성\n",
    "train_mask = np.isin(y_train_full, [1, 7])\n",
    "X_train, y_train = X_train_full[train_mask], y_train_full[train_mask]\n",
    "\n",
    "test_mask = np.isin(y_test_full, [1, 7])\n",
    "X_test, y_test = X_test_full[test_mask], y_test_full[test_mask]\n",
    "\n",
    "# 시드 고정 (예: 42로 고정)\n",
    "np.random.seed(42)\n",
    "\n",
    "# 2000개의 데이터를 무작위로 선택\n",
    "num_samples = 2000\n",
    "indices = np.random.choice(len(X_train), num_samples, replace=False)\n",
    "X_sampled, y_sampled = X_train[indices], y_train[indices]\n",
    "\n",
    "# 2000개의 샘플에서 train/test 데이터 분할 (80% train, 20% test 비율로 나눔)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_sampled, y_sampled, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 결과\n",
    "print(f\"Training set size: {len(X_train)}\")\n",
    "print(f\"Test set size: {len(X_test)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.datasets import load_digits\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.feature_extraction.image import extract_patches_2d\n",
    "\n",
    "# digits = load_digits()\n",
    "# X, y = digits.images, digits.target\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "# train_mask = np.isin(y_train, [1, 7])\n",
    "# X_train, y_train = X_train[train_mask], y_train[train_mask]\n",
    "\n",
    "# test_mask = np.isin(y_test, [1, 7])\n",
    "# X_test, y_test = X_test[test_mask], y_test[test_mask]\n",
    "\n",
    "# #\n",
    "# # X_train = X_train.reshape(X_train.shape[0], 16, 4)\n",
    "# # X_test = X_test.reshape(X_test.shape[0], 16, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1600, 28, 28)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2x2 크기로 겹치지 않게 패치로 나누는 함수\n",
    "def split_into_non_overlapping_patches(image, patch_size=(4, 4)):\n",
    "    patches = []\n",
    "    for i in range(0, image.shape[0], patch_size[0]):\n",
    "        for j in range(0, image.shape[1], patch_size[1]):\n",
    "            patch = image[i:i+patch_size[0], j:j+patch_size[1]].flatten()\n",
    "            patches.append(patch)\n",
    "    return np.array(patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 이미지를 2x2 겹치지 않는 패치로 나누기\n",
    "X_train = np.array([split_into_non_overlapping_patches(img) for img in X_train])\n",
    "X_test = np.array([split_into_non_overlapping_patches(img) for img in X_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (283, 16, 2, 2)를 (283, 16, 4)로 변환\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], -1)\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 이미지 패치 나누기 (2x2 크기 패치)\n",
    "# patch_size = (2, 2)\n",
    "\n",
    "# # 각 이미지를 개별적으로 패치로 나누기\n",
    "# X_train = np.array([extract_patches_2d(img, patch_size) for img in X_train])\n",
    "# X_test = np.array([extract_patches_2d(img, patch_size) for img in X_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1600, 49, 16)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # def circuit_v(self,inputs,weights):\n",
    "    #         op=self.random_op()\n",
    "    #         # feature_map\n",
    "    #         indx=0\n",
    "    #         for j in range(self.num_q):\n",
    "    #             qml.RX(inputs[indx],j)\n",
    "    #             qml.RY(inputs[indx+1],j)\n",
    "    #             indx+=2\n",
    "    #         for i in range(self.Denc):\n",
    "    #             for j in range(self.num_q):\n",
    "    #                 qml.CNOT(wires=(j,(j+1)%self.num_q))\n",
    "\n",
    "    #             for j in range(self.num_q):\n",
    "    #                 qml.RY(inputs[indx],j)\n",
    "    #                 indx+=1\n",
    "    #         # Ansatz\n",
    "    #         indx=0\n",
    "    #         for j in range(self.num_q):\n",
    "    #             qml.RX(weights[indx],j)\n",
    "    #             qml.RY(weights[indx+1],j)\n",
    "    #             indx+=2\n",
    "    #         for i in range(self.D):\n",
    "    #             for j in range(self.num_q):\n",
    "    #                 qml.CNOT(wires=(j,(j+1)%self.num_q))\n",
    "                    \n",
    "    #             for j in range(self.num_q):\n",
    "    #                 #qc.rx(params[indx],j)\n",
    "    #                 qml.RY(weights[indx],j)\n",
    "    #                 indx+=1\n",
    "    #         return [qml.expval(op) for i in range(self.d)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=QSANN_text_classifier(49, 4, 2, 1, 1)\n",
    "\n",
    "# seq * (num_qubits) * (num_layers + 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(lr=0.01, params=model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2549\n"
     ]
    }
   ],
   "source": [
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(trainable_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = (torch.round(torch.sign(preds-0.5))+1)//2\n",
    "    correct = (rounded_preds == y).float() #convert into float for division \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1600, 49, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jaeyeob/anaconda3/envs/lstm/lib/python3.10/site-packages/autoray/autoray.py:81: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  return func(*args, **kwargs)\n",
      "  0%|          | 0/30 [1:15:29<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha: tensor([[[0.9998, 0.8984, 0.9981,  ..., 0.9931, 0.9945, 0.9887],\n",
      "         [0.9984, 0.9283, 0.9904,  ..., 0.9992, 0.9835, 0.9973],\n",
      "         [0.9967, 0.9365, 0.9869,  ..., 0.9999, 0.9790, 0.9987],\n",
      "         ...,\n",
      "         [0.9990, 0.9234, 0.9921,  ..., 0.9986, 0.9858, 0.9962],\n",
      "         [0.9969, 0.9358, 0.9872,  ..., 0.9998, 0.9794, 0.9986],\n",
      "         [0.9099, 0.6804, 0.9397,  ..., 0.8681, 0.9533, 0.8523]],\n",
      "\n",
      "        [[0.9988, 0.8968, 0.9915,  ..., 0.9949, 0.9949, 0.9881],\n",
      "         [0.9984, 0.8932, 0.9904,  ..., 0.9940, 0.9957, 0.9868],\n",
      "         [0.9971, 0.8852, 0.9876,  ..., 0.9918, 0.9972, 0.9836],\n",
      "         ...,\n",
      "         [0.9998, 0.9233, 0.9980,  ..., 0.9994, 0.9859, 0.9962],\n",
      "         [0.9969, 0.8839, 0.9872,  ..., 0.9915, 0.9975, 0.9831],\n",
      "         [0.9999, 0.9225, 0.9979,  ..., 0.9994, 0.9862, 0.9960]],\n",
      "\n",
      "        [[0.9976, 0.8984, 0.9920,  ..., 0.9953, 0.9945, 0.9893],\n",
      "         [1.0000, 0.9283, 0.9988,  ..., 0.9998, 0.9835, 0.9976],\n",
      "         [0.9945, 0.8830, 0.9869,  ..., 0.9912, 0.9976, 0.9835],\n",
      "         ...,\n",
      "         [1.0000, 0.9234, 0.9980,  ..., 0.9994, 0.9858, 0.9966],\n",
      "         [0.9947, 0.8839, 0.9872,  ..., 0.9915, 0.9975, 0.9838],\n",
      "         [0.8895, 0.6804, 0.8637,  ..., 0.8773, 0.9533, 0.8542]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[1.0000, 0.9255, 0.9983,  ..., 0.9949, 0.9696, 0.9966],\n",
      "         [0.9967, 0.8935, 0.9904,  ..., 0.9998, 0.9861, 0.9868],\n",
      "         [0.9995, 0.9368, 0.9997,  ..., 0.9912, 0.9614, 0.9987],\n",
      "         ...,\n",
      "         [0.9977, 0.8992, 0.9921,  ..., 0.9994, 0.9838, 0.9889],\n",
      "         [0.9997, 0.9345, 0.9995,  ..., 0.9921, 0.9632, 0.9984],\n",
      "         [0.9193, 0.9999, 0.9397,  ..., 0.8773, 0.8047, 0.9474]],\n",
      "\n",
      "        [[0.9997, 0.9255, 0.9983,  ..., 0.9988, 0.9949, 0.9881],\n",
      "         [0.9999, 0.9206, 0.9975,  ..., 0.9981, 0.9935, 0.9901],\n",
      "         [0.9987, 0.9351, 0.9995,  ..., 0.9998, 0.9972, 0.9836],\n",
      "         ...,\n",
      "         [0.9991, 0.8992, 0.9922,  ..., 0.9933, 0.9859, 0.9962],\n",
      "         [0.9973, 0.8862, 0.9879,  ..., 0.9893, 0.9803, 0.9984],\n",
      "         [0.8998, 0.6809, 0.8637,  ..., 0.8681, 0.8429, 0.9474]],\n",
      "\n",
      "        [[1.0000, 0.9238, 0.9984,  ..., 0.9995, 0.9840, 0.9887],\n",
      "         [1.0000, 0.9283, 0.9990,  ..., 0.9998, 0.9861, 0.9868],\n",
      "         [0.9995, 0.9365, 0.9998,  ..., 1.0000, 0.9897, 0.9827],\n",
      "         ...,\n",
      "         [0.9977, 0.8989, 0.9928,  ..., 0.9954, 0.9708, 0.9962],\n",
      "         [0.9996, 0.9358, 0.9997,  ..., 1.0000, 0.9894, 0.9831],\n",
      "         [0.9978, 0.8999, 0.9931,  ..., 0.9956, 0.9714, 0.9960]]],\n",
      "       grad_fn=<TransposeBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 2401 into shape (4,4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m      3\u001b[0m X_tensor\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mtensor(X_train)\n\u001b[0;32m----> 4\u001b[0m predictions\u001b[39m=\u001b[39mmodel(X_tensor\u001b[39m.\u001b[39;49mfloat())\u001b[39m.\u001b[39msqueeze(\u001b[39m1\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[39m#predictions=torch.sign(predictions)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39m#print(predictions)\u001b[39;00m\n\u001b[1;32m      7\u001b[0m label\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mtensor(y_train)\n",
      "File \u001b[0;32m~/anaconda3/envs/lstm/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[4], line 189\u001b[0m, in \u001b[0;36mQSANN_text_classifier.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m,\u001b[39minput\u001b[39m):\n\u001b[1;32m    188\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39minput\u001b[39m\u001b[39m.\u001b[39mshape)\n\u001b[0;32m--> 189\u001b[0m     x\u001b[39m=\u001b[39m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mQnn(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    190\u001b[0m     x\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mflatten(x,start_dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    191\u001b[0m     \u001b[39m# print('done2')\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/lstm/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[4], line 169\u001b[0m, in \u001b[0;36mQSANN_pennylane.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m,\u001b[39minput\u001b[39m):\n\u001b[0;32m--> 169\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mqnn(\u001b[39minput\u001b[39;49m)\n",
      "File \u001b[0;32m~/anaconda3/envs/lstm/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/lstm/lib/python3.10/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    138\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    140\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/lstm/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[4], line 134\u001b[0m, in \u001b[0;36mQSAL_pennylane.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39malpha: \u001b[39m\u001b[39m{\u001b[39;00malpha\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m    133\u001b[0m \u001b[39m# Example usage:\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m visualize_attention(alpha)\n\u001b[1;32m    135\u001b[0m V_output\u001b[39m=\u001b[39mV_output\u001b[39m.\u001b[39mtranspose(\u001b[39m0\u001b[39m,\u001b[39m1\u001b[39m)\n\u001b[1;32m    136\u001b[0m output\u001b[39m=\u001b[39m[]\n",
      "Cell \u001b[0;32mIn[3], line 13\u001b[0m, in \u001b[0;36mvisualize_attention\u001b[0;34m(alpha, img_shape, patch_shape)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39m# Reshape alpha to match image shape\u001b[39;00m\n\u001b[1;32m     12\u001b[0m alpha \u001b[39m=\u001b[39m alpha\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy() \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(alpha, torch\u001b[39m.\u001b[39mTensor) \u001b[39melse\u001b[39;00m alpha\n\u001b[0;32m---> 13\u001b[0m attention_map \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mmean(alpha, axis\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\u001b[39m.\u001b[39;49mreshape(patch_shape)  \u001b[39m# Mean over the sequence\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[39m# Normalize the attention map\u001b[39;00m\n\u001b[1;32m     16\u001b[0m attention_map \u001b[39m=\u001b[39m (attention_map \u001b[39m-\u001b[39m attention_map\u001b[39m.\u001b[39mmin()) \u001b[39m/\u001b[39m (attention_map\u001b[39m.\u001b[39mmax() \u001b[39m-\u001b[39m attention_map\u001b[39m.\u001b[39mmin())\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 2401 into shape (4,4)"
     ]
    }
   ],
   "source": [
    "for iepoch in tqdm(range(30)):\n",
    "    optimizer.zero_grad()\n",
    "    X_tensor=torch.tensor(X_train)\n",
    "    predictions=model(X_tensor.float()).squeeze(1)\n",
    "    #predictions=torch.sign(predictions)\n",
    "    #print(predictions)\n",
    "    label=torch.tensor(y_train)\n",
    "    for i in range(len(label)):\n",
    "        if label[i]==1:\n",
    "            label[i] = 0\n",
    "        else:\n",
    "            label[i]=1\n",
    "    #print(label)\n",
    "    loss = criterion(predictions, label.float())\n",
    "    acc = binary_accuracy(predictions, label)\n",
    "    print('')\n",
    "    print('Accuracy:',acc)\n",
    "    print('')\n",
    "    print(loss)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(\"Complete \"+str(iepoch)+\" th\")\n",
    "    \n",
    "    # 1epoch 75분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([78, 4, 16])\n",
      "alpha: tensor([[[0.9324, 0.9693, 0.7342, 0.8715],\n",
      "         [0.9520, 0.9531, 0.7685, 0.8979],\n",
      "         [0.9869, 0.7343, 0.9693, 0.9999],\n",
      "         [0.9969, 0.8619, 0.8867, 0.9742]],\n",
      "\n",
      "        [[0.8979, 0.8744, 0.9648, 0.9989],\n",
      "         [0.9854, 0.9931, 0.9344, 0.7919],\n",
      "         [0.9823, 0.9709, 1.0000, 0.9494],\n",
      "         [0.9953, 0.9990, 0.9577, 0.8308]],\n",
      "\n",
      "        [[0.9745, 0.9143, 0.9790, 0.9994],\n",
      "         [0.9998, 0.9846, 0.9991, 0.9610],\n",
      "         [0.9933, 0.9524, 0.9955, 0.9894],\n",
      "         [0.7811, 0.6675, 0.7926, 0.9073]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.7962, 0.7265, 0.8644, 0.8931],\n",
      "         [0.8782, 0.8180, 0.9323, 0.9531],\n",
      "         [0.9865, 0.9591, 0.9996, 0.9994],\n",
      "         [0.9989, 0.9855, 0.9961, 0.9884]],\n",
      "\n",
      "        [[0.7414, 0.8363, 0.8019, 0.7218],\n",
      "         [0.9995, 0.9790, 0.9903, 1.0000],\n",
      "         [0.7259, 0.6211, 0.6613, 0.7454],\n",
      "         [0.9979, 0.9939, 0.9990, 0.9952]],\n",
      "\n",
      "        [[0.9926, 0.9659, 0.9569, 0.9933],\n",
      "         [0.9854, 0.9774, 0.9417, 0.9864],\n",
      "         [0.9930, 0.8810, 0.9984, 0.9923],\n",
      "         [0.9861, 0.8584, 1.0000, 0.9850]]], grad_fn=<TransposeBackward0>)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q_output size: torch.Size([4, 78, 4])\n",
      "K_output size: torch.Size([4, 78, 4])\n",
      "V_output size: torch.Size([78, 4, 16])\n",
      "\n",
      "Accuracy: 1.0\n",
      "\n",
      "Loss: 164.04408264160156\n",
      "\n",
      "AUROC: 1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7033/3021993959.py:7: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  rounded_preds = (torch.round(torch.sign(preds-0.5))+1)//2\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "X_tensor=torch.tensor(X_test)\n",
    "predictions=model(X_tensor.float()).squeeze(1)\n",
    "label=torch.tensor(y_test)\n",
    "for i in range(len(label)):\n",
    "        if label[i]==1:\n",
    "            label[i] = 0\n",
    "        else:\n",
    "            label[i]=1\n",
    "loss = criterion(predictions, label.float())\n",
    "acc = binary_accuracy(predictions, label.float())\n",
    "\n",
    "# Calculate AUROC\n",
    "preds_np = predictions.detach().numpy()  # Detach predictions from the graph and convert to numpy\n",
    "labels_np = label.numpy()  # Convert labels to numpy\n",
    "auroc = roc_auc_score(labels_np, preds_np)\n",
    "\n",
    "# Print results\n",
    "print(f'\\nAccuracy: {acc}\\n')\n",
    "print(f'Loss: {loss}\\n')\n",
    "print(f'AUROC: {auroc}\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quantum_multi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
