{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-24 22:19:07.558692: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-10-24 22:19:07.560857: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-10-24 22:19:07.707203: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-10-24 22:19:07.956652: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-24 22:19:08.426762: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-24 22:19:08.457392: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-24 22:19:08.597357: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-24 22:19:46.051591: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 160\n",
      "Test set size: 40\n",
      "Sampled train labels: [0 1]\n",
      "Sampled test labels: [0 1]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import cifar10\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# CIFAR-10 데이터 불러오기\n",
    "(X_train_full, y_train_full), (X_test_full, y_test_full) = cifar10.load_data()\n",
    "\n",
    "# 데이터 정규화 (0-255 값을 0-1 사이로)\n",
    "X_train_full = X_train_full.astype('float32') / 255.0\n",
    "X_test_full = X_test_full.astype('float32') / 255.0\n",
    "\n",
    "# RGB -> Grayscale 변환\n",
    "# 공식: 0.299*R + 0.587*G + 0.114*B\n",
    "X_train_gray = np.dot(X_train_full[...,:3], [0.299, 0.587, 0.114])\n",
    "X_test_gray = np.dot(X_test_full[...,:3], [0.299, 0.587, 0.114])\n",
    "\n",
    "# 원하는 두 개의 클래스만 선택 (예: 클래스 0과 1)\n",
    "selected_classes = [0, 1]\n",
    "\n",
    "# 클래스 0과 1에 해당하는 데이터만 선택 (train set)\n",
    "train_mask = np.isin(y_train_full, selected_classes)\n",
    "X_train_filtered = X_train_gray[train_mask.squeeze()]\n",
    "y_train_filtered = y_train_full[train_mask.squeeze()]\n",
    "\n",
    "# 클래스 0과 1에 해당하는 데이터만 선택 (test set)\n",
    "test_mask = np.isin(y_test_full, selected_classes)\n",
    "X_test_filtered = X_test_gray[test_mask.squeeze()]\n",
    "y_test_filtered = y_test_full[test_mask.squeeze()]\n",
    "\n",
    "# 클래스 라벨을 이진 라벨로 변환 (0 또는 1로)\n",
    "y_train_filtered = (y_train_filtered == selected_classes[1]).astype(int)\n",
    "y_test_filtered = (y_test_filtered == selected_classes[1]).astype(int)\n",
    "\n",
    "# 시드 고정\n",
    "np.random.seed(42)\n",
    "\n",
    "# 2000개의 데이터를 무작위로 선택\n",
    "num_samples = 200\n",
    "indices = np.random.choice(len(X_train_filtered), num_samples, replace=False)\n",
    "X_sampled, y_sampled = X_train_filtered[indices], y_train_filtered[indices]\n",
    "\n",
    "# 2000개의 샘플에서 train/test 데이터 분할 (80% train, 20% test 비율로 나눔)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_sampled, y_sampled, stratify=y_sampled, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "y_train = y_train.squeeze(1)\n",
    "y_test = y_test.squeeze(1)\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"Training set size: {len(X_train)}\")\n",
    "print(f\"Test set size: {len(X_test)}\")\n",
    "print(f\"Sampled train labels: {np.unique(y_train)}\")\n",
    "print(f\"Sampled test labels: {np.unique(y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2x2 크기로 겹치지 않게 패치로 나누는 함수\n",
    "def split_into_non_overlapping_patches(image, patch_size=(4, 4)):\n",
    "    patches = []\n",
    "    for i in range(0, image.shape[0], patch_size[0]):\n",
    "        for j in range(0, image.shape[1], patch_size[1]):\n",
    "            patch = image[i:i+patch_size[0], j:j+patch_size[1]].flatten()\n",
    "            patches.append(patch)\n",
    "    return np.array(patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 이미지를 2x2 겹치지 않는 패치로 나누기\n",
    "X_train = np.array([split_into_non_overlapping_patches(img) for img in X_train])\n",
    "X_test = np.array([split_into_non_overlapping_patches(img) for img in X_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 80, 1: 80}\n"
     ]
    }
   ],
   "source": [
    "# y_train이 numpy 배열일 경우\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "class_distribution = dict(zip(unique, counts))\n",
    "print(class_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 20, 1: 20}\n"
     ]
    }
   ],
   "source": [
    "# y_train이 numpy 배열일 경우\n",
    "unique, counts = np.unique(y_test, return_counts=True)\n",
    "class_distribution = dict(zip(unique, counts))\n",
    "print(class_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(160, 64, 16)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# Self-Attention and Binary Classifier from previous code\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        self.head_dim = embed_size // heads\n",
    "\n",
    "        assert self.head_dim * heads == embed_size, \"Embedding size must be divisible by heads\"\n",
    "\n",
    "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.fc_out = nn.Linear(embed_size, embed_size)\n",
    "\n",
    "    def forward(self, values, keys, query, mask=None):\n",
    "        N = query.shape[0]\n",
    "        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
    "\n",
    "        # Split embedding into multiple heads\n",
    "        values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
    "        keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
    "        queries = query.reshape(N, query_len, self.heads, self.head_dim)\n",
    "\n",
    "        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
    "\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "\n",
    "        attention = torch.softmax(energy / (self.embed_size ** (1 / 2)), dim=3)\n",
    "\n",
    "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n",
    "            N, query_len, self.heads * self.head_dim\n",
    "        )\n",
    "\n",
    "        out = self.fc_out(out)\n",
    "        return out\n",
    "\n",
    "# Classifier model\n",
    "class SimpleSelfAttentionClassifier(nn.Module):\n",
    "    def __init__(self, embed_size=16, heads=2, num_classes=2):\n",
    "        super(SimpleSelfAttentionClassifier, self).__init__()\n",
    "        self.attention = SelfAttention(embed_size, heads)\n",
    "        self.fc = nn.Linear(embed_size * 64, num_classes)  # 49 is the sequence length in X_train\n",
    "\n",
    "    def forward(self, x):\n",
    "        attention_output = self.attention(x, x, x)\n",
    "        attention_output = attention_output.flatten(start_dim=1)  # Flatten to feed into classifier\n",
    "        out = self.fc(attention_output)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model, loss, optimizer\n",
    "model = SimpleSelfAttentionClassifier(embed_size=16, heads=2)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def binary_accuracy(preds, y):\n",
    "#     \"\"\"\n",
    "#     Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "#     \"\"\"\n",
    "\n",
    "#     #round predictions to the closest integer\n",
    "#     rounded_preds = (torch.round(torch.sign(preds-0.5))+1)//2\n",
    "#     correct = (rounded_preds == y).float() #convert into float for division \n",
    "#     acc = correct.sum() / len(correct)\n",
    "#     return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your binary accuracy function\n",
    "def binary_accuracy(preds, y):\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float()\n",
    "    acc = correct.sum() / len(correct)\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5\n",
      "Loss: 0.6940199136734009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1/30 [00:08<04:14,  8.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 complete\n",
      "Accuracy: 0.5\n",
      "Loss: 0.7460027933120728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 2/30 [00:17<04:09,  8.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 complete\n",
      "Accuracy: 0.5\n",
      "Loss: 0.6931542754173279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 3/30 [00:24<03:31,  7.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 complete\n",
      "Accuracy: 0.5\n",
      "Loss: 0.7023259401321411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 4/30 [00:29<02:58,  6.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 complete\n",
      "Accuracy: 0.5\n",
      "Loss: 0.7163920402526855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 5/30 [00:35<02:44,  6.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 complete\n",
      "Accuracy: 0.5\n",
      "Loss: 0.7029472589492798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 6/30 [00:38<02:09,  5.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 complete\n",
      "Accuracy: 0.5687500238418579\n",
      "Loss: 0.6861727237701416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 7/30 [00:43<01:56,  5.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 complete\n",
      "Accuracy: 0.53125\n",
      "Loss: 0.6836047172546387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 8/30 [00:48<01:52,  5.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 complete\n",
      "Accuracy: 0.518750011920929\n",
      "Loss: 0.6910464763641357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 9/30 [00:52<01:41,  4.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 complete\n",
      "Accuracy: 0.518750011920929\n",
      "Loss: 0.6940042972564697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 10/30 [00:57<01:34,  4.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 complete\n",
      "Accuracy: 0.5249999761581421\n",
      "Loss: 0.6881883144378662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 11/30 [01:05<01:51,  5.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 complete\n",
      "Accuracy: 0.5375000238418579\n",
      "Loss: 0.6797626614570618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 12/30 [01:11<01:42,  5.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 complete\n",
      "Accuracy: 0.625\n",
      "Loss: 0.6758752465248108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 13/30 [01:16<01:35,  5.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 complete\n",
      "Accuracy: 0.5687500238418579\n",
      "Loss: 0.6779056787490845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 14/30 [01:19<01:19,  4.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 complete\n",
      "Accuracy: 0.5625\n",
      "Loss: 0.6809606552124023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 15/30 [01:24<01:11,  4.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 complete\n",
      "Accuracy: 0.5625\n",
      "Loss: 0.6801573038101196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 16/30 [01:31<01:16,  5.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 complete\n",
      "Accuracy: 0.5625\n",
      "Loss: 0.6757946014404297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 17/30 [01:35<01:06,  5.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 complete\n",
      "Accuracy: 0.606249988079071\n",
      "Loss: 0.6717162728309631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 18/30 [01:39<00:55,  4.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 complete\n",
      "Accuracy: 0.6312500238418579\n",
      "Loss: 0.6708482503890991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 19/30 [01:44<00:52,  4.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 complete\n",
      "Accuracy: 0.5625\n",
      "Loss: 0.672507107257843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 20/30 [01:51<00:53,  5.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 complete\n",
      "Accuracy: 0.5625\n",
      "Loss: 0.6736078262329102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 21/30 [01:56<00:49,  5.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 complete\n",
      "Accuracy: 0.5625\n",
      "Loss: 0.6722851395606995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 22/30 [02:01<00:41,  5.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22 complete\n",
      "Accuracy: 0.6000000238418579\n",
      "Loss: 0.6695563197135925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 23/30 [02:07<00:37,  5.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23 complete\n",
      "Accuracy: 0.637499988079071\n",
      "Loss: 0.6677523851394653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 24/30 [02:13<00:34,  5.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24 complete\n",
      "Accuracy: 0.606249988079071\n",
      "Loss: 0.6679868102073669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 25/30 [02:20<00:30,  6.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25 complete\n",
      "Accuracy: 0.581250011920929\n",
      "Loss: 0.669097900390625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 26/30 [02:25<00:23,  5.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26 complete\n",
      "Accuracy: 0.581250011920929\n",
      "Loss: 0.6691707968711853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 27/30 [02:30<00:16,  5.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27 complete\n",
      "Accuracy: 0.5874999761581421\n",
      "Loss: 0.6678140163421631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 28/30 [02:35<00:10,  5.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28 complete\n",
      "Accuracy: 0.612500011920929\n",
      "Loss: 0.666343629360199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 29/30 [02:38<00:04,  4.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29 complete\n",
      "Accuracy: 0.6312500238418579\n",
      "Loss: 0.6660415530204773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [02:41<00:00,  5.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30 complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Training the model\n",
    "for iepoch in tqdm(range(30)):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Convert your X_train into tensor\n",
    "    X_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "    \n",
    "    # Forward pass\n",
    "    predictions = model(X_tensor)\n",
    "    \n",
    "    # Convert y_train to tensor and change labels from 1 to 0, and 7 to 1\n",
    "    label = torch.tensor(y_train, dtype=torch.long).clone()  # Ensure labels are LongTensor\n",
    "    for i in range(len(label)):\n",
    "        label[i] = 0 if label[i] == 1 else 1  # 1 -> 0, 7 -> 1\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = criterion(predictions, label)\n",
    "    \n",
    "    # Calculate binary accuracy (adjusted for your binary classification)\n",
    "    acc = binary_accuracy(predictions.argmax(dim=1), label)\n",
    "    \n",
    "    # Print accuracy and loss\n",
    "    print(f'Accuracy: {acc}')\n",
    "    print(f'Loss: {loss.item()}')\n",
    "    \n",
    "    # Backward pass and optimization step\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch {iepoch+1} complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy: 0.625\n",
      "\n",
      "Loss: 0.6493237614631653\n",
      "\n",
      "AUROC: 0.685\n",
      "\n",
      "Precision: 0.631578947368421\n",
      "\n",
      "Recall: 0.6\n",
      "\n",
      "F1 Score: 0.6153846153846154\n",
      "\n",
      "AUPRC: 0.6729369268219667\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_fscore_support, average_precision_score\n",
    "\n",
    "# Convert test data to tensor\n",
    "X_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "predictions = model(X_tensor)\n",
    "\n",
    "# Convert y_train to tensor and change labels from 1 to 0, and 7 to 1\n",
    "label = torch.tensor(y_test, dtype=torch.long).clone()  # Ensure labels are LongTensor\n",
    "        \n",
    "# Reverse the labels (if needed)\n",
    "for i in range(len(label)):\n",
    "    if label[i] == 1:\n",
    "        label[i] = 0\n",
    "    else:\n",
    "        label[i] = 1\n",
    "\n",
    "# Calculate loss and accuracy\n",
    "# Compute loss\n",
    "loss = criterion(predictions, label)\n",
    "\n",
    "# Calculate binary accuracy (adjusted for your binary classification)\n",
    "acc = binary_accuracy(predictions.argmax(dim=1), label)\n",
    "\n",
    "# Convert predictions to probabilities using softmax\n",
    "probabilities = torch.softmax(predictions, dim=1)  # Shape: (batch_size, 2)\n",
    "\n",
    "# Extract probabilities for class 1 (positive class)\n",
    "positive_class_probs = probabilities[:, 1].detach().numpy()\n",
    "\n",
    "# Convert labels to numpy\n",
    "labels_np = label.numpy()\n",
    "\n",
    "# Calculate AUROC\n",
    "auroc = roc_auc_score(labels_np, positive_class_probs)\n",
    "\n",
    "# Binarize predictions for precision, recall, and F1 calculation\n",
    "binary_preds = np.where(positive_class_probs > 0.5, 1, 0)\n",
    "\n",
    "# Calculate Precision, Recall, and F1 Score\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(labels_np, binary_preds, average='binary')\n",
    "\n",
    "# Calculate AUPRC (Area Under the Precision-Recall Curve)\n",
    "auprc = average_precision_score(labels_np, positive_class_probs)\n",
    "\n",
    "# Print results\n",
    "print(f'\\nAccuracy: {acc}\\n')\n",
    "print(f'Loss: {loss}\\n')\n",
    "print(f'AUROC: {auroc}\\n')\n",
    "print(f'Precision: {precision}\\n')\n",
    "print(f'Recall: {recall}\\n')\n",
    "print(f'F1 Score: {f1}\\n')\n",
    "print(f'AUPRC: {auprc}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lstm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
